{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/disaster/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/disaster/test.csv\")\n",
    "df_train.drop(columns=[\"target\"], inplace=True)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - 21/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(tweets):\n",
    "    return list(map(word_tokenize, tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', 'to', \"'shelter\", 'in', 'place', \"'\", 'are', 'being', 'notified', 'by', 'officers', '.', 'No', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_tweets):\n",
    "    _function = lambda tweet: [word for word in tweet if word not in stopwords]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_corpus = remove_stopwords(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'Reason', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'No', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(tokenized_tweets):\n",
    "    inverted_index = dict()\n",
    "    for document_idx in range(len(tokenized_tweets)):\n",
    "        for word in tokenized_tweets[document_idx]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = list()\n",
    "            inverted_index[word].append(document_idx)\n",
    "    \n",
    "    for key in inverted_index:\n",
    "        inverted_index[key] = list(set(inverted_index[key]))\n",
    "        \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: Deeds\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: Reason\n",
      "Tweet Indices: [0, 8610, 304, 305, 317, 319]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - 28/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(tokenized_tweets):\n",
    "    _function = lambda tweet: [word.lower() for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_corpus = case_folding(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deeds', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'us']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'no', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [lemmatizer.lemmatize(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_lemm_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_lemm_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [stemmer.stem(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_stem_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_stem_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created new Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_processed = create_inverted_index(tokenized_nostopw_case_lemm_corpus) # tokenized_nostopw_case_stem_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: deed\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: reason\n",
      "Tweet Indices: [0, 1920, 10496, 4997, 5000, 5001, 9737, 9739, 781, 8593, 5372, 6166, 7961, 10526, 8224, 8610, 10670, 304, 305, 7218, 6453, 9911, 2747, 6459, 4669, 317, 319, 2112, 7740, 8001, 8260, 2252, 6991, 6232, 6242, 746, 4843, 4333, 6898, 4084, 763, 3452, 894, 4991]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index_processed.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index_processed[key]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
