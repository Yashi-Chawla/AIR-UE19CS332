{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ac95c7",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204a5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f037c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def2b5f",
   "metadata": {},
   "source": [
    "# Initialising Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde06b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12f644",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b41d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/disaster/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/disaster/test.csv\")\n",
    "df_train.drop(columns=[\"target\"], inplace=True)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a55967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a860b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe9677",
   "metadata": {},
   "source": [
    "# Week 1 - 21/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923de243",
   "metadata": {},
   "source": [
    "## Tokenizing Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d164cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(tweets):\n",
    "    return list(map(word_tokenize, tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05d7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae07d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', 'to', \"'shelter\", 'in', 'place', \"'\", 'are', 'being', 'notified', 'by', 'officers', '.', 'No', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a324f9",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94188e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_tweets):\n",
    "    _function = lambda tweet: [word for word in tweet if word not in stopwords]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fdd866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_corpus = remove_stopwords(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9feec068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'Reason', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'No', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79734e",
   "metadata": {},
   "source": [
    "## Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e382a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(tokenized_tweets):\n",
    "    inverted_index = dict()\n",
    "    for document_idx in range(len(tokenized_tweets)):\n",
    "        for word in tokenized_tweets[document_idx]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = list()\n",
    "            inverted_index[word].append(document_idx)\n",
    "    \n",
    "    for key in inverted_index:\n",
    "        inverted_index[key] = list(set(inverted_index[key]))\n",
    "        \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7727e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9b9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: Deeds\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: Reason\n",
      "Tweet Indices: [0, 8610, 304, 305, 317, 319]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68f5e1",
   "metadata": {},
   "source": [
    "# Week 2 - 28/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe24237",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b458bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(tokenized_tweets):\n",
    "    _function = lambda tweet: [word.lower() for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f37a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_corpus = case_folding(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58520269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deeds', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'us']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'no', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4f21d",
   "metadata": {},
   "source": [
    "## Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa58f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [lemmatizer.lemmatize(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7105536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_lemm_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f1520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_lemm_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5549a7",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d8512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [stemmer.stem(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_stem_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c986210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_stem_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fa2bc",
   "metadata": {},
   "source": [
    "## Created new Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26d27a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_processed = create_inverted_index(tokenized_nostopw_case_lemm_corpus) # tokenized_nostopw_case_stem_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfeecaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: deed\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: reason\n",
      "Tweet Indices: [0, 1920, 10496, 4997, 5000, 5001, 9737, 9739, 781, 8593, 5372, 6166, 7961, 10526, 8224, 8610, 10670, 304, 305, 7218, 6453, 9911, 2747, 6459, 4669, 317, 319, 2112, 7740, 8001, 8260, 2252, 6991, 6232, 6242, 746, 4843, 4333, 6898, 4084, 763, 3452, 894, 4991]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index_processed.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index_processed[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952b6f3",
   "metadata": {},
   "source": [
    "# Week 3 - 04/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc8a39",
   "metadata": {},
   "source": [
    "## Boolean Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84b9675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(infix_tokens):\n",
    "    \"\"\" Parse Query \n",
    "    Parsing done using Shunting Yard Algorithm \n",
    "    \"\"\"\n",
    "    precedence = {}\n",
    "    precedence['NOT'] = 3\n",
    "    precedence['AND'] = 2\n",
    "    precedence['OR'] = 1\n",
    "    precedence['('] = 0\n",
    "    precedence[')'] = 0    \n",
    "\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "\n",
    "    for token in infix_tokens:\n",
    "        if (token == '('):\n",
    "            operator_stack.append(token)\n",
    "\n",
    "        # if right bracket, pop all operators from operator stack onto output until we hit left bracket\n",
    "        elif (token == ')'):\n",
    "            operator = operator_stack.pop()\n",
    "            while operator != '(':\n",
    "                output.append(operator)\n",
    "                operator = operator_stack.pop()\n",
    "\n",
    "        # if operator, pop operators from operator stack to queue if they are of higher precedence\n",
    "        elif (token in precedence):\n",
    "            # if operator stack is not empty\n",
    "            if (operator_stack):\n",
    "                current_operator = operator_stack[-1]\n",
    "                while (operator_stack and precedence[current_operator] > precedence[token]):\n",
    "                    output.append(operator_stack.pop())\n",
    "                    if (operator_stack):\n",
    "                        current_operator = operator_stack[-1]\n",
    "            operator_stack.append(token) # add token to stack\n",
    "        else:\n",
    "            output.append(token.lower())\n",
    "\n",
    "    # while there are still operators on the stack, pop them into the queue\n",
    "    while (operator_stack):\n",
    "        output.append(operator_stack.pop())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b5c356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(query, inverted_index):\n",
    "    query = query.strip()\n",
    "    query_tokens = query.split()\n",
    "    boolean_query = parse_query(query_tokens)\n",
    "        \n",
    "    result_stack = list()\n",
    "    for idx, token in enumerate(boolean_query):\n",
    "        if token not in [\"AND\", \"NOT\", \"OR\"]:\n",
    "            result = set(inverted_index[token])\n",
    "        else:\n",
    "            if token in ['AND', 'OR']:\n",
    "                right_operand = result_stack.pop()\n",
    "                left_operand = result_stack.pop()\n",
    "                \n",
    "                if token == 'AND':\n",
    "                    operation = set.intersection\n",
    "                else:\n",
    "                    operation = set.union\n",
    "                \n",
    "                result = operation(left_operand, right_operand)\n",
    "                \n",
    "            else:\n",
    "                operand = result_stack.pop()\n",
    "                complement_document_ids = inverted_index[boolean_query[idx-1]]\n",
    "                result = list()\n",
    "                for word in inverted_index:\n",
    "                    result.extend([_id for _id in inverted_index[word] if _id not in complement_document_ids])\n",
    "                result = set(result)\n",
    "                \n",
    "        result_stack.append(result)\n",
    "    \n",
    "    return result_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8254b2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs: {0}\n"
     ]
    }
   ],
   "source": [
    "document_ids = boolean_query(\"our AND deed\", inverted_index_processed)\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb91a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs: {0, 4480, 2435, 4483, 2696, 2827, 6795, 7438, 6034, 3987, 9503, 9504, 6564, 1710, 1712, 2484, 4534, 1600, 2373, 2376, 2377, 5450, 10443, 8652, 8654, 2386, 2390, 471, 2392, 2391, 9302, 6627, 8677, 6631, 9579, 4468, 6776, 10493}\n"
     ]
    }
   ],
   "source": [
    "document_ids = boolean_query(\"our AND deed OR india\", inverted_index_processed)\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c512f",
   "metadata": {},
   "source": [
    "## Phrase Query with Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760514d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
