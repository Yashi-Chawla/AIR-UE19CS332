{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ac95c7",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204a5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f037c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditeya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def2b5f",
   "metadata": {},
   "source": [
    "# Initialising Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde06b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12f644",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b41d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/disaster/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/disaster/test.csv\")\n",
    "df_train.drop(columns=[\"target\"], inplace=True)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a55967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a860b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe9677",
   "metadata": {},
   "source": [
    "# Week 1 - 21/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923de243",
   "metadata": {},
   "source": [
    "## Tokenizing Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d164cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(tweets):\n",
    "    return list(map(word_tokenize, tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05d7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae07d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', 'to', \"'shelter\", 'in', 'place', \"'\", 'are', 'being', 'notified', 'by', 'officers', '.', 'No', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a324f9",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94188e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_tweets):\n",
    "    _function = lambda tweet: [word for word in tweet if word not in stopwords]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fdd866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_corpus = remove_stopwords(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9feec068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['Our', 'Deeds', 'Reason', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us']\n",
      "Tweet 1: ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', '.', 'Canada']\n",
      "Tweet 2: ['All', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'No', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79734e",
   "metadata": {},
   "source": [
    "## Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e382a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(tokenized_tweets):\n",
    "    inverted_index = dict()\n",
    "    for document_idx in range(len(tokenized_tweets)):\n",
    "        for word in tokenized_tweets[document_idx]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = list()\n",
    "            inverted_index[word].append(document_idx)\n",
    "    \n",
    "    for key in inverted_index:\n",
    "        inverted_index[key] = list(set(inverted_index[key]))\n",
    "        \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7727e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9b9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: Deeds\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: Reason\n",
      "Tweet Indices: [0, 8610, 304, 305, 317, 319]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68f5e1",
   "metadata": {},
   "source": [
    "# Week 2 - 28/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe24237",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b458bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(tokenized_tweets):\n",
    "    _function = lambda tweet: [word.lower() for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f37a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_corpus = case_folding(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58520269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deeds', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'us']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'residents', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officers', '.', 'no', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4f21d",
   "metadata": {},
   "source": [
    "## Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa58f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [lemmatizer.lemmatize(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7105536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_lemm_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f1520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_lemm_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5549a7",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d8512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [stemmer.stem(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_stem_corpus = lemmatize_words(tokenized_nostopw_case_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c986210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['our', 'deed', 'reason', '#', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "Tweet 1: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']\n",
      "Tweet 2: ['all', 'resident', 'asked', \"'shelter\", 'place', \"'\", 'notified', 'officer', '.', 'no', 'evacuation', 'shelter', 'place', 'order', 'expected']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_stem_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fa2bc",
   "metadata": {},
   "source": [
    "## Created new Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26d27a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_processed = create_inverted_index(tokenized_nostopw_case_lemm_corpus) # tokenized_nostopw_case_stem_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfeecaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: our\n",
      "Tweet Indices: [0, 9095, 7816, 1673, 1550, 8080, 3345, 8979, 4630, 3099, 2976, 3618, 3235, 2855, 6567, 3369, 2220, 4659, 3124, 4024, 4281, 3786, 10829, 1371, 9570, 3172, 10603, 1645, 4209, 7157, 4987, 2431]\n",
      "\n",
      "Word: deed\n",
      "Tweet Indices: [0, 4985]\n",
      "\n",
      "Word: reason\n",
      "Tweet Indices: [0, 1920, 10496, 4997, 5000, 5001, 9737, 9739, 781, 8593, 5372, 6166, 7961, 10526, 8224, 8610, 10670, 304, 305, 7218, 6453, 9911, 2747, 6459, 4669, 317, 319, 2112, 7740, 8001, 8260, 2252, 6991, 6232, 6242, 746, 4843, 4333, 6898, 4084, 763, 3452, 894, 4991]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(inverted_index_processed.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet Indices: {inverted_index_processed[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952b6f3",
   "metadata": {},
   "source": [
    "# Week 3 - 04/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc8a39",
   "metadata": {},
   "source": [
    "## Boolean Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84b9675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(infix_tokens):\n",
    "    \"\"\" Parse Query \n",
    "    Parsing done using Shunting Yard Algorithm \n",
    "    \"\"\"\n",
    "    precedence = {}\n",
    "    precedence['NOT'] = 3\n",
    "    precedence['AND'] = 2\n",
    "    precedence['OR'] = 1\n",
    "    precedence['('] = 0\n",
    "    precedence[')'] = 0    \n",
    "\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "\n",
    "    for token in infix_tokens:\n",
    "        if (token == '('):\n",
    "            operator_stack.append(token)\n",
    "\n",
    "        # if right bracket, pop all operators from operator stack onto output until we hit left bracket\n",
    "        elif (token == ')'):\n",
    "            operator = operator_stack.pop()\n",
    "            while operator != '(':\n",
    "                output.append(operator)\n",
    "                operator = operator_stack.pop()\n",
    "\n",
    "        # if operator, pop operators from operator stack to queue if they are of higher precedence\n",
    "        elif (token in precedence):\n",
    "            # if operator stack is not empty\n",
    "            if (operator_stack):\n",
    "                current_operator = operator_stack[-1]\n",
    "                while (operator_stack and precedence[current_operator] > precedence[token]):\n",
    "                    output.append(operator_stack.pop())\n",
    "                    if (operator_stack):\n",
    "                        current_operator = operator_stack[-1]\n",
    "            operator_stack.append(token) # add token to stack\n",
    "        else:\n",
    "            output.append(token.lower())\n",
    "\n",
    "    # while there are still operators on the stack, pop them into the queue\n",
    "    while (operator_stack):\n",
    "        output.append(operator_stack.pop())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b5c356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(query, inverted_index):\n",
    "    query = query.strip()\n",
    "    query_tokens = query.split()\n",
    "    boolean_query = parse_query(query_tokens)\n",
    "        \n",
    "    result_stack = list()\n",
    "    for idx, token in enumerate(boolean_query):\n",
    "        if token not in [\"AND\", \"NOT\", \"OR\"]:\n",
    "            result = set(inverted_index[token])\n",
    "        else:\n",
    "            if token in ['AND', 'OR']:\n",
    "                right_operand = result_stack.pop()\n",
    "                left_operand = result_stack.pop()\n",
    "                \n",
    "                if token == 'AND':\n",
    "                    operation = set.intersection\n",
    "                else:\n",
    "                    operation = set.union\n",
    "                \n",
    "                result = operation(left_operand, right_operand)\n",
    "                \n",
    "            else:\n",
    "                operand = result_stack.pop()\n",
    "                complement_document_ids = inverted_index[boolean_query[idx-1]]\n",
    "                result = list()\n",
    "                for word in inverted_index:\n",
    "                    result.extend([_id for _id in inverted_index[word] if _id not in complement_document_ids])\n",
    "                result = set(result)\n",
    "                \n",
    "        result_stack.append(result)\n",
    "    \n",
    "    return result_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8254b2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs: {0}\n"
     ]
    }
   ],
   "source": [
    "document_ids = boolean_query(\"our AND deed\", inverted_index_processed)\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cb91a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs: {0, 4480, 2435, 4483, 2696, 2827, 6795, 7438, 6034, 3987, 9503, 9504, 6564, 1710, 1712, 2484, 4534, 1600, 2373, 2376, 2377, 5450, 10443, 8652, 8654, 2386, 2390, 471, 2392, 2391, 9302, 6627, 8677, 6631, 9579, 4468, 6776, 10493}\n"
     ]
    }
   ],
   "source": [
    "document_ids = boolean_query(\"our AND deed OR india\", inverted_index_processed)\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c512f",
   "metadata": {},
   "source": [
    "## Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "760514d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_positional_posting_list(tokenized_corpus):\n",
    "    positional_index = dict()\n",
    "    for tweet_id, tweet in enumerate(tokenized_corpus):\n",
    "        for token_id, token in enumerate(tweet):\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = dict()\n",
    "            if tweet_id not in positional_index[token]:\n",
    "                positional_index[token][tweet_id] = list()\n",
    "            positional_index[token][tweet_id].append(token_id)\n",
    "    \n",
    "    for token in positional_index:\n",
    "        for tweet_id in positional_index[token]:\n",
    "            positional_index[token][tweet_id] = sorted(positional_index[token][tweet_id])\n",
    "        items = list(positional_index[token].items())\n",
    "        items.sort(key=lambda x: x[0])\n",
    "        for k, v in items:\n",
    "            positional_index[token][k] = v\n",
    "            \n",
    "    return positional_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d5a8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index = construct_positional_posting_list(tokenized_nostopw_case_lemm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c206db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: our\n",
      "Tweet & Token Indices: {0: [0], 1371: [6], 1550: [0], 1645: [5], 1673: [4], 2220: [8], 2431: [4], 2855: [11], 2976: [7], 3099: [12], 3124: [12], 3172: [0], 3235: [0], 3345: [0], 3369: [0], 3618: [5], 3786: [0], 4024: [0], 4209: [0], 4281: [8], 4630: [0], 4659: [6], 4987: [8], 6567: [10], 7157: [4], 7816: [16, 22], 8080: [0], 8979: [8], 9095: [0], 9570: [0], 10603: [14], 10829: [0]}\n",
      "\n",
      "Word: deed\n",
      "Tweet & Token Indices: {0: [1], 4985: [9]}\n",
      "\n",
      "Word: reason\n",
      "Tweet & Token Indices: {0: [2], 304: [19], 305: [19], 317: [19], 319: [19], 746: [4], 763: [3], 781: [7], 894: [8], 1920: [0], 2112: [10], 2252: [3], 2747: [12], 3452: [0], 4084: [7], 4333: [7], 4669: [2], 4843: [8], 4991: [1], 4997: [1], 5000: [1], 5001: [1], 5372: [13], 6166: [0], 6232: [1], 6242: [3], 6453: [9], 6459: [2], 6898: [11], 6991: [0], 7218: [9], 7740: [5], 7961: [1], 8001: [2], 8224: [7], 8260: [6], 8593: [9], 8610: [16], 9737: [1], 9739: [1], 9911: [9], 10496: [7], 10526: [3], 10670: [2]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in list(positional_index.keys())[:3]:\n",
    "    print(f\"Word: {key}\\nTweet & Token Indices: {positional_index[key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b0441",
   "metadata": {},
   "source": [
    "## Phrase Query with Postional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "245ce3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect(p1, p2, K):\n",
    "    answer = list()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(p1) and j < len(p2):\n",
    "        document_id_p1 = list(p1.keys())[i]\n",
    "        document_id_p2 = list(p2.keys())[j]\n",
    "        \n",
    "        if document_id_p1 == document_id_p2:\n",
    "            l = list()\n",
    "            pp1 = p1[document_id_p1]\n",
    "            pp2 = p2[document_id_p2]\n",
    "            \n",
    "            k = 0\n",
    "            while k < len(pp1):\n",
    "                m = 0\n",
    "                while m < len(pp2):\n",
    "#                     distance = abs(pp1[k] - pp2[m])\n",
    "                    distance = pp2[m] - pp1[k]\n",
    "                    if distance == K:\n",
    "#                     if distance <= K:\n",
    "                        l.append(m)\n",
    "#                     elif pp2[m] > pp1[k]:\n",
    "#                         break\n",
    "                    m += 1\n",
    "                \n",
    "                for ps in l:\n",
    "                    distance = (pp2[ps] - pp1[k])\n",
    "                    if distance != K:\n",
    "                        l.remove(ps)\n",
    "                        \n",
    "                for ps in l:\n",
    "                    answer.append((document_id_p1, pp1[k], pp2[ps]))\n",
    "                \n",
    "                k += 1\n",
    "            \n",
    "            i += 1\n",
    "            j += 1\n",
    "        \n",
    "        elif document_id_p1 < document_id_p2:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f5a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to = {\n",
    "    1: [7,18,33,72,86,231],\n",
    "    2: [1,17,74,222,255],\n",
    "    4: [8,16,190,429,433],\n",
    "    5: [363,367],\n",
    "    7: [13,23,191]\n",
    "}\n",
    "        \n",
    "be = {\n",
    "    1: [17,25],\n",
    "    4: [17,191,291,430,434],\n",
    "    5: [14,19,101]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e075fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 16, 17), (4, 190, 191), (4, 429, 430), (4, 433, 434)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_intersect(to, be, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04d0ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_positional_index(query):\n",
    "    query = query.split()\n",
    "    words = [query[i] for i in range(0, len(query), 2)]\n",
    "    k = [int(query[i][1:]) for i in range(1, len(query), 2)]\n",
    "    \n",
    "    document_list = list()\n",
    "    for i in range(0, len(words)-1):\n",
    "        word1, word2 = words[i:i+2]\n",
    "        p1 = positional_index[word1]\n",
    "        p2 = positional_index[word2]\n",
    "        result = positional_intersect(p1, p2, k[i])\n",
    "        # print(result)\n",
    "        # result = list(map(lambda x: x[0], result))\n",
    "        document_list.extend(result)\n",
    "        \n",
    "    return document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98e0f053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4921, 4, 5),\n",
       " (4933, 4, 5),\n",
       " (5099, 3, 4),\n",
       " (7788, 6, 7),\n",
       " (8614, 4, 5),\n",
       " (9695, 4, 5),\n",
       " (9706, 4, 5),\n",
       " (9707, 4, 5),\n",
       " (9712, 4, 5),\n",
       " (9806, 8, 9)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_positional_index(\"to /1 be\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
