{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a02988",
   "metadata": {},
   "source": [
    "# Loading Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a27aa1",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c11bfb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0601fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ba7e0",
   "metadata": {},
   "source": [
    "## Preprocessing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a529534",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa408c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = word_tokenize(article.lower().strip())\n",
    "    article = [\n",
    "        lemmatizer.lemmatize(w)\n",
    "        for w in article \n",
    "            if w not in stopwords and w not in string.punctuation\n",
    "    ]\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b639684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(articles):\n",
    "    vocabulary = set()\n",
    "    for article in articles:\n",
    "        for word in article:\n",
    "            vocabulary.add(word)\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    return (vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84818d",
   "metadata": {},
   "source": [
    "## BST Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c91fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, word, document_id):\n",
    "        self.word = word\n",
    "        self.postings = [document_id]\n",
    "        self.document_frequency = 1\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def add_doc_to_word_posting(self, document_id):\n",
    "        if document_id not in self.postings:\n",
    "            self.postings.append(document_id)\n",
    "            self.document_frequency += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61bfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BST:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def insert(self, word, document_id):\n",
    "        if self.root is None:\n",
    "            self.root = Node(word, document_id)\n",
    "            return\n",
    "        else:\n",
    "            self.insert_word(self.root, word, document_id)\n",
    "    \n",
    "    def insert_word(self, node, word, document_id):\n",
    "        if word < node.word:\n",
    "            if node.left is None:\n",
    "                node.left = Node(word, document_id)\n",
    "            else:\n",
    "                self.insert_word(node.left, word, document_id)\n",
    "        elif word > node.word:\n",
    "            if node.right is None:\n",
    "                node.right = Node(word, document_id)\n",
    "            else:\n",
    "                self.insert_word(node.right, word, document_id)\n",
    "        else:\n",
    "            node.add_doc_to_word_posting(document_id)\n",
    "\n",
    "    def search(self, word):\n",
    "        if self.root:\n",
    "            return self.search_word(self.root, word)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def search_word(self, node, word):\n",
    "        if word < node.word:\n",
    "            if node.left is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self.search_word(node.left, word)\n",
    "        elif word > node.word:\n",
    "            if node.right is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self.search_word(node.right, word)\n",
    "        else:\n",
    "            return (node.postings, node.document_frequency)\n",
    "\n",
    "#     def inorder(self):\n",
    "#         if self.root:\n",
    "#             self.inorder_traversal(self.root)\n",
    "\n",
    "#     def inorder_traversal(self, node):\n",
    "#         if node.left:\n",
    "#             self.inorder_traversal(node.left)\n",
    "#         print(f'{node.word} {node.postings} {node.document_frequency}')\n",
    "#         if node.right:\n",
    "#             self.inorder_traversal(node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba6d5e",
   "metadata": {},
   "source": [
    "# Creating Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7e71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 s, sys: 129 ms, total: 47.6 s\n",
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocessed_articles = list(map(preprocess, newsgroup['data']))\n",
    "document_ids = list(newsgroup['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46e3eb",
   "metadata": {},
   "source": [
    "## Creating BST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee07d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_tree(articles, document_ids):\n",
    "    search_tree = BST()\n",
    "    for i in tqdm(range(len(articles))):\n",
    "        article = articles[i]\n",
    "        document_id = document_ids[i]\n",
    "        for word in article:\n",
    "            search_tree.insert(word, document_id)\n",
    "    return search_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d52c48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0843ce401f4042879537faca97a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 170 ms, total: 13.4 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "search_tree = create_search_tree(preprocessed_articles, document_ids)\n",
    "bst_creation_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e08ab",
   "metadata": {},
   "source": [
    "## Creating Hash Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3948bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_table(articles, document_ids):\n",
    "    # vocabulary, vocabulary_size = get_vocabulary(articles)\n",
    "    # hash_table_size = vocabulary_size\n",
    "    hash_table_size = 100000\n",
    "    hash_table = [list() for _ in range(hash_table_size)]\n",
    "    for i in tqdm(range(len(articles))):\n",
    "        article = articles[i]\n",
    "        document_id = document_ids[i]\n",
    "        for word in article:\n",
    "            hash_idx = hash(word) % hash_table_size\n",
    "            if not hash_table[hash_idx]: # if spot is empty\n",
    "                hash_table[hash_idx].append([word, 1, [document_id]])\n",
    "                \n",
    "                \n",
    "            else: # if spot is full\n",
    "                word_present = False # check word is already there. If yes, increment doc freq, add doc_id if needed\n",
    "                for j in range(len(hash_table[hash_idx])):\n",
    "                    chain_word = hash_table[hash_idx][j][0]\n",
    "                    if chain_word == word:\n",
    "                        word_present = True\n",
    "                        if document_id not in hash_table[hash_idx][j][2]:\n",
    "                            hash_table[hash_idx][j][2].append(document_id)\n",
    "                            hash_table[hash_idx][j][1] += 1\n",
    "                        break\n",
    "                \n",
    "                if not word_present: # word not present in chain\n",
    "                    hash_table[hash_idx].append([word, 1, [document_id]])\n",
    "    \n",
    "    return hash_table, hash_table_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251e260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ae7f9df50b462ba3f5c19b67a72337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.13 s, sys: 70.6 ms, total: 4.2 s\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "hash_table, hash_table_size = create_hash_table(preprocessed_articles, document_ids)\n",
    "hash_table_creation_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a9d08",
   "metadata": {},
   "source": [
    "# Querying Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3efc87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, vocabulary_size = get_vocabulary(preprocessed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e6481",
   "metadata": {},
   "source": [
    "## Querying BST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf07dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 19, 1, 13, 15, 14, 18, 0, 11]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = search_tree.search(\"india\")\n",
    "search_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece6956",
   "metadata": {},
   "source": [
    "### Average Query Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7b1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_times = list()\n",
    "for word in vocabulary:\n",
    "    start_time = time.time()\n",
    "    search_result = search_tree.search(word)\n",
    "    query_duration = time.time() - start_time\n",
    "    query_times.append(query_duration)\n",
    "bst_query_duration = np.mean(query_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98cf87",
   "metadata": {},
   "source": [
    "## Querying Hash Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd762c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hash_table(hash_table, hash_table_size, word):\n",
    "    hash_idx = hash(word) % hash_table_size\n",
    "    for chain_word in hash_table[hash_idx]:\n",
    "        if chain_word[0] == word:\n",
    "            return chain_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a257bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 19, 1, 13, 15, 14, 18, 0, 11]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = query_hash_table(hash_table, hash_table_size, \"india\")\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73c8a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_times = list()\n",
    "for word in vocabulary:\n",
    "    start_time = time.time()\n",
    "    search_result = query_hash_table(hash_table, hash_table_size, word)\n",
    "    query_duration = time.time() - start_time\n",
    "    query_times.append(query_duration)\n",
    "hash_table_query_duration = np.mean(query_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26569ce4",
   "metadata": {},
   "source": [
    "# Comparison between BST and Hash Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae747bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Creation Time (seconds)</th>\n",
       "      <th>Query Time (μ seconds)</th>\n",
       "      <th>Memory Size (bytes)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hash Table</th>\n",
       "      <td>4.155758</td>\n",
       "      <td>1.463309</td>\n",
       "      <td>800968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BST</th>\n",
       "      <td>13.275200</td>\n",
       "      <td>6.756076</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Creation Time (seconds)  Query Time (μ seconds)  \\\n",
       "Method                                                        \n",
       "Hash Table                 4.155758                1.463309   \n",
       "BST                       13.275200                6.756076   \n",
       "\n",
       "            Memory Size (bytes)  \n",
       "Method                           \n",
       "Hash Table               800968  \n",
       "BST                          32  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame()\n",
    "comparison_df[\"Method\"] = ['Hash Table', 'BST']\n",
    "comparison_df.set_index(\"Method\", inplace=True)\n",
    "comparison_df[\"Creation Time (seconds)\"] = [hash_table_creation_duration, bst_creation_duration]\n",
    "comparison_df[\"Query Time (μ seconds)\"] = [hash_table_query_duration * 10 ** 6, bst_query_duration * 10 ** 6]\n",
    "comparison_df[\"Memory Size (bytes)\"] = [hash_table.__sizeof__(), search_tree.__sizeof__()]\n",
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
