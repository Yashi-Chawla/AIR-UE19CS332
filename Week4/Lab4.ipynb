{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a43fac",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8dfbb",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7236facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb93bab",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449726f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c6c68",
   "metadata": {},
   "source": [
    "## Preprocessing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd6839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d47a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = word_tokenize(article.lower().strip())\n",
    "    article = [\n",
    "        lemmatizer.lemmatize(w.translate(str.maketrans('', '', string.punctuation)))\n",
    "        for w in article \n",
    "            if w not in stopwords and w not in string.punctuation\n",
    "    ]\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f5ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(articles):\n",
    "    vocabulary = set()\n",
    "    for article in articles:\n",
    "        for word in article:\n",
    "            vocabulary.add(word)\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    return (vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e66dbd",
   "metadata": {},
   "source": [
    "# Permuterm Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7658c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_articles = list(map(preprocess, newsgroup['data']))\n",
    "document_ids = list(newsgroup['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323e888",
   "metadata": {},
   "source": [
    "## Obtaining Permuterm Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2f3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_permutations(word):\n",
    "    word = f'{word.strip()}$'\n",
    "    permutations = list()\n",
    "    for i in range(len(word)):\n",
    "        permute = word[i:] + word[:i]\n",
    "        permutations.append(permute)\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab5455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_index = dict()\n",
    "for article in preprocessed_articles:\n",
    "    for token in article:\n",
    "        if token not in permutation_index:\n",
    "            permutation_index[token] = get_word_permutations(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a2a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello$', 'ello$h', 'llo$he', 'lo$hel', 'o$hell', '$hello']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_index['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6f488",
   "metadata": {},
   "source": [
    "## BST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180ead54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, word, document_id):\n",
    "        self.word = word\n",
    "        self.postings = [document_id]\n",
    "        self.document_frequency = 1\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def add_doc_to_word_posting(self, document_id):\n",
    "        if document_id not in self.postings:\n",
    "            self.postings.append(document_id)\n",
    "            self.document_frequency += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb711b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BST:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def insert(self, word, document_id):\n",
    "        if self.root is None:\n",
    "            self.root = Node(word, document_id)\n",
    "            return\n",
    "        else:\n",
    "            self.insert_word(self.root, word, document_id)\n",
    "    \n",
    "    def insert_word(self, node, word, document_id):\n",
    "        if word < node.word:\n",
    "            if node.left is None:\n",
    "                node.left = Node(word, document_id)\n",
    "            else:\n",
    "                self.insert_word(node.left, word, document_id)\n",
    "        elif word > node.word:\n",
    "            if node.right is None:\n",
    "                node.right = Node(word, document_id)\n",
    "            else:\n",
    "                self.insert_word(node.right, word, document_id)\n",
    "        else:\n",
    "            node.add_doc_to_word_posting(document_id)\n",
    "\n",
    "    def search(self, word):\n",
    "        if self.root:\n",
    "            return self.search_word(self.root, word)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def search_word(self, node, word):\n",
    "        if word < node.word:\n",
    "            if node.left is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self.search_word(node.left, word)\n",
    "        elif word > node.word:\n",
    "            if node.right is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self.search_word(node.right, word)\n",
    "        else:\n",
    "            return (node.postings, node.document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdccd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_tree(articles, document_ids):\n",
    "    search_tree = BST()\n",
    "    for i in tqdm(range(len(articles))):\n",
    "        article = articles[i]\n",
    "        document_id = document_ids[i]\n",
    "        for word in article:\n",
    "            search_tree.insert(word, document_id)\n",
    "    return search_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675a1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca57559abf44a019bb50fd7cdffa4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_tree = create_search_tree(preprocessed_articles, document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89207562",
   "metadata": {},
   "source": [
    "## Searching Query with Permuterm Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d9eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_permuterm(query):\n",
    "    if '*' in query:\n",
    "        query = f'{query.strip()}$'\n",
    "        for i in range(len(query)):\n",
    "            permute = query[i:] + query[:i]\n",
    "            if permute[-1] == '*':\n",
    "                break\n",
    "        query = permute\n",
    "\n",
    "        query = list(filter(bool, query.split('*')))\n",
    "        search_word = list(filter(lambda x: '$' in x, query))[0]\n",
    "        filter_words = list(filter(lambda x: not('$' in x), query))\n",
    "\n",
    "        search_words = set()\n",
    "        for token in permutation_index:\n",
    "            if token == query:\n",
    "                filter_flag = [w in token for w in filter_words]\n",
    "                if all(filter_flag):\n",
    "                    search_words.add(token)\n",
    "            else:\n",
    "                for permute in permutation_index[token]:\n",
    "                    if search_word in permute:\n",
    "                        filter_flag = [w in permute for w in filter_words]\n",
    "                        if all(filter_flag):\n",
    "                            search_words.add(token)\n",
    "        search_words = list(search_words)\n",
    "    else:\n",
    "        search_words = [query]\n",
    "    \n",
    "    print(search_words)\n",
    "    posting_lists = [set(search_tree.search(w)[0]) for w in search_words]\n",
    "    posting_list = set.intersection(*posting_lists)\n",
    "    print(posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abacd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'indonesia']\n",
      "{0, 18, 13, 14}\n"
     ]
    }
   ],
   "source": [
    "search_permuterm(\"ind*ia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e19bd",
   "metadata": {},
   "source": [
    "# K-Gram Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532b50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c5751",
   "metadata": {},
   "source": [
    "## Obtaining K-Gram Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b97966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kgrams(word, k):\n",
    "    word = f'${word.strip()}$'\n",
    "    kgrams = ngrams(word, k)\n",
    "    kgrams = list(map(lambda x: ''.join(x), kgrams))\n",
    "    return kgrams\n",
    "\n",
    "\n",
    "def create_kgram_index(articles, k=3):\n",
    "    kgram_index = dict()\n",
    "    for article in articles:\n",
    "        for token in article:\n",
    "            kgrams = get_kgrams(token, k)\n",
    "            for kgram in kgrams:\n",
    "                if kgram not in kgram_index:\n",
    "                    kgram_index[kgram] = set()\n",
    "                kgram_index[kgram].add(token)\n",
    "                \n",
    "    for kgram in kgram_index:\n",
    "        kgram_index[kgram] = sorted(list(kgram_index[kgram]))\n",
    "    \n",
    "    return kgram_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6999dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kgram_index = create_kgram_index(preprocessed_articles, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5583427",
   "metadata": {},
   "source": [
    "## Searching with K-Gram Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a1716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_kgram(query):\n",
    "    if '*' in query:\n",
    "        query_regex = query.replace('*', '.*')\n",
    "        query_kgrams = get_kgrams(query, K)\n",
    "        query_kgrams = list(filter(lambda x: not('*' in x), query_kgrams))\n",
    "        search_words = list()\n",
    "        for query_kgram in query_kgrams:\n",
    "            search_words.append(set(kgram_index[query_kgram]))\n",
    "        \n",
    "        search_words = list(set.intersection(*search_words))\n",
    "        search_words = [w for w in search_words if re.match(query_regex, w).span()[1] == len(w)]\n",
    "        \n",
    "    else:\n",
    "        search_words = [query]\n",
    "    \n",
    "    print(search_words)\n",
    "    posting_lists = [set(search_tree.search(w)[0]) for w in search_words]\n",
    "    posting_list = set.intersection(*posting_lists)\n",
    "    print(posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b5183eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'indonesia']\n",
      "{0, 18, 13, 14}\n"
     ]
    }
   ],
   "source": [
    "search_kgram('ind*ia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97b7f8",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b7f24",
   "metadata": {},
   "source": [
    "## Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f23f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(articles):\n",
    "    for article in articles:\n",
    "        for token in article:\n",
    "            yield token\n",
    "            \n",
    "def reducer(articles):\n",
    "    mapper_function = mapper(articles)\n",
    "    collection_frequency = dict()\n",
    "    while True:\n",
    "        try:\n",
    "            token = mapper_function.__next__()\n",
    "            if token not in collection_frequency:\n",
    "                collection_frequency[token] = 0\n",
    "            collection_frequency[token] += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return collection_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6adcadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_frequency = reducer(preprocessed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0805f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: mamatha\n",
      "Frequency: 12\n",
      "\n",
      "Word: devineni\n",
      "Frequency: 12\n",
      "\n",
      "Word: ratnam\n",
      "Frequency: 16\n",
      "\n",
      "Word: mr47\n",
      "Frequency: 12\n",
      "\n",
      "Word: andrewcmuedu\n",
      "Frequency: 495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, frequency in list(collection_frequency.items())[:5]:\n",
    "    print(f'Word: {token}\\nFrequency: {frequency}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbad705",
   "metadata": {},
   "source": [
    "## Collection Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f656575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_frequency(articles):\n",
    "    collection_frequency = dict()\n",
    "    for article in articles:\n",
    "        for token in article:\n",
    "            if token not in collection_frequency:\n",
    "                collection_frequency[token] = 0\n",
    "            collection_frequency[token] += 1\n",
    "         \n",
    "    # collection_frequency = list(collection_frequency.items())\n",
    "    # collection_frequency.sort(key=lambda x: x[0])\n",
    "    # collection_frequency = dict(collection_frequency)\n",
    "    return collection_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcb0960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_frequency = get_collection_frequency(preprocessed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29e4c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: mamatha\n",
      "Frequency: 12\n",
      "\n",
      "Word: devineni\n",
      "Frequency: 12\n",
      "\n",
      "Word: ratnam\n",
      "Frequency: 16\n",
      "\n",
      "Word: mr47\n",
      "Frequency: 12\n",
      "\n",
      "Word: andrewcmuedu\n",
      "Frequency: 495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, frequency in list(collection_frequency.items())[:5]:\n",
    "    print(f'Word: {token}\\nFrequency: {frequency}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
